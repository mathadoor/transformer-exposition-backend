2023-10-04T17:28:06,627 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2023-10-04T17:28:06,627 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2023-10-04T17:28:06,653 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2023-10-04T17:28:06,653 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2023-10-04T17:28:06,653 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2023-10-04T17:28:06,653 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2023-10-04T17:28:06,679 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml
2023-10-04T17:28:06,679 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml
2023-10-04T17:28:06,792 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.8.2
TS Home: /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages
Current directory: /home/mathadoor/Documents/transformer-exposition-backend/app
Temp directory: /tmp
Metrics config path: /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 32
Max heap size: 16000 M
Python executable: /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/bin/python3.9
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/mathadoor/Documents/transformer-exposition-backend/app
Initial Models: TinyTransformer.mar
Log dir: /home/mathadoor/Documents/transformer-exposition-backend/app/logs
Metrics dir: /home/mathadoor/Documents/transformer-exposition-backend/app/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: log
Disable system metrics: false
Workflow Store: /home/mathadoor/Documents/transformer-exposition-backend/app
Model config: N/A
2023-10-04T17:28:06,792 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.8.2
TS Home: /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages
Current directory: /home/mathadoor/Documents/transformer-exposition-backend/app
Temp directory: /tmp
Metrics config path: /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 32
Max heap size: 16000 M
Python executable: /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/bin/python3.9
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/mathadoor/Documents/transformer-exposition-backend/app
Initial Models: TinyTransformer.mar
Log dir: /home/mathadoor/Documents/transformer-exposition-backend/app/logs
Metrics dir: /home/mathadoor/Documents/transformer-exposition-backend/app/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: log
Disable system metrics: false
Workflow Store: /home/mathadoor/Documents/transformer-exposition-backend/app
Model config: N/A
2023-10-04T17:28:06,801 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: TinyTransformer.mar
2023-10-04T17:28:06,801 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: TinyTransformer.mar
2023-10-04T17:28:06,975 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model TinyTransformer
2023-10-04T17:28:06,975 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model TinyTransformer
2023-10-04T17:28:06,975 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model TinyTransformer
2023-10-04T17:28:06,975 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model TinyTransformer
2023-10-04T17:28:06,975 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model TinyTransformer loaded.
2023-10-04T17:28:06,975 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model TinyTransformer loaded.
2023-10-04T17:28:06,975 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: TinyTransformer, count: 1
2023-10-04T17:28:06,975 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: TinyTransformer, count: 1
2023-10-04T17:28:06,979 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/bin/python3.9, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml]
2023-10-04T17:28:06,979 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/bin/python3.9, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml]
2023-10-04T17:28:06,980 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2023-10-04T17:28:06,980 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2023-10-04T17:28:07,012 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2023-10-04T17:28:07,012 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2023-10-04T17:28:07,012 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2023-10-04T17:28:07,012 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2023-10-04T17:28:07,013 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2023-10-04T17:28:07,013 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2023-10-04T17:28:07,013 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2023-10-04T17:28:07,013 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2023-10-04T17:28:07,013 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2023-10-04T17:28:07,013 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2023-10-04T17:28:07,129 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2023-10-04T17:28:07,129 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2023-10-04T17:28:07,163 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2023-10-04T17:28:07,163 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2023-10-04T17:28:07,682 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=48870
2023-10-04T17:28:07,682 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-10-04T17:28:07,709 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Successfully loaded /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml.
2023-10-04T17:28:07,709 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - [PID]48870
2023-10-04T17:28:07,710 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Torch worker started.
2023-10-04T17:28:07,710 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Python runtime: 3.9.18
2023-10-04T17:28:07,710 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change null -> WORKER_STARTED
2023-10-04T17:28:07,710 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change null -> WORKER_STARTED
2023-10-04T17:28:07,713 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-10-04T17:28:07,713 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-10-04T17:28:07,716 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2023-10-04T17:28:07,718 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD to backend at: 1696454887718
2023-10-04T17:28:07,718 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD to backend at: 1696454887718
2023-10-04T17:28:07,739 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - model_name: TinyTransformer, batchSize: 1
2023-10-04T17:28:07,766 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - proceeding without onnxruntime
2023-10-04T17:28:07,766 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2023-10-04T17:28:08,252 [WARN ] W-9000-TinyTransformer_1.0-stderr MODEL_LOG - /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
2023-10-04T17:28:08,253 [WARN ] W-9000-TinyTransformer_1.0-stderr MODEL_LOG -   warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
2023-10-04T17:28:08,274 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - sent a reply, jobdone: true
2023-10-04T17:28:08,274 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - sent a reply, jobdone: true
2023-10-04T17:28:08,275 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 535
2023-10-04T17:28:08,275 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 535
2023-10-04T17:28:08,275 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2023-10-04T17:28:08,275 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change WORKER_STARTED -> WORKER_MODEL_LOADED
2023-10-04T17:28:08,275 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:1297.0|#WorkerName:W-9000-TinyTransformer_1.0,Level:Host|#hostname:berserker,timestamp:1696454888
2023-10-04T17:28:08,275 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:22.0|#Level:Host|#hostname:berserker,timestamp:1696454888
2023-10-04T17:28:11,549 [INFO ] epollEventLoopGroup-3-1 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:TinyTransformer,model_version:default|#hostname:berserker,timestamp:1696454891
2023-10-04T17:28:11,550 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1696454891550
2023-10-04T17:28:11,550 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1696454891550
2023-10-04T17:28:11,551 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - extra_files <ts.context.Context object at 0x7f6c05cfe4c0>
2023-10-04T17:28:11,551 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Backend received inference at: 1696454891
2023-10-04T17:28:11,584 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - model_input KDog.
2023-10-04T17:28:11,584 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]HandlerTime.Milliseconds:33.13|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,1696454891,f6722068-390e-4758-9429-d54aa85efafe, pattern=[METRICS]
2023-10-04T17:28:11,584 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]HandlerTime.Milliseconds:33.13|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,1696454891,f6722068-390e-4758-9429-d54aa85efafe, pattern=[METRICS]
2023-10-04T17:28:11,585 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_METRICS - HandlerTime.ms:33.13|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,requestID:f6722068-390e-4758-9429-d54aa85efafe,timestamp:1696454891
2023-10-04T17:28:11,585 [INFO ] W-9000-TinyTransformer_1.0 ACCESS_LOG - /127.0.0.1:42138 "POST /predictions/TinyTransformer HTTP/1.1" 200 37
2023-10-04T17:28:11,585 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:33.25|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,1696454891,f6722068-390e-4758-9429-d54aa85efafe, pattern=[METRICS]
2023-10-04T17:28:11,585 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:berserker,timestamp:1696454891
2023-10-04T17:28:11,585 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:33.25|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,1696454891,f6722068-390e-4758-9429-d54aa85efafe, pattern=[METRICS]
2023-10-04T17:28:11,586 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - ts_inference_latency_microseconds.Microseconds:35104.415|#model_name:TinyTransformer,model_version:default|#hostname:berserker,timestamp:1696454891
2023-10-04T17:28:11,586 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_METRICS - PredictionTime.ms:33.25|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,requestID:f6722068-390e-4758-9429-d54aa85efafe,timestamp:1696454891
2023-10-04T17:28:11,586 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - ts_queue_latency_microseconds.Microseconds:257.42|#model_name:TinyTransformer,model_version:default|#hostname:berserker,timestamp:1696454891
2023-10-04T17:28:11,586 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.job.Job - Waiting time ns: 257420, Backend time ns: 36036167
2023-10-04T17:28:11,586 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.job.Job - Waiting time ns: 257420, Backend time ns: 36036167
2023-10-04T17:28:11,586 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - QueueTime.Milliseconds:0.0|#Level:Host|#hostname:berserker,timestamp:1696454891
2023-10-04T17:28:11,586 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - sent a reply, jobdone: true
2023-10-04T17:28:11,586 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - sent a reply, jobdone: true
2023-10-04T17:28:11,586 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 34
2023-10-04T17:28:11,586 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 34
2023-10-04T17:28:11,586 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:2.0|#Level:Host|#hostname:berserker,timestamp:1696454891
2023-10-04T17:28:23,018 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:TinyTransformer,model_version:default|#hostname:berserker,timestamp:1696454903
2023-10-04T17:28:23,019 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1696454903019
2023-10-04T17:28:23,019 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1696454903019
2023-10-04T17:28:23,020 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Backend received inference at: 1696454903
2023-10-04T17:28:23,050 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - model_input KThe dog.
2023-10-04T17:28:23,050 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]HandlerTime.Milliseconds:30.0|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,1696454903,9ce35cb4-4d60-431b-90a5-a61b1c2dfb4c, pattern=[METRICS]
2023-10-04T17:28:23,050 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]HandlerTime.Milliseconds:30.0|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,1696454903,9ce35cb4-4d60-431b-90a5-a61b1c2dfb4c, pattern=[METRICS]
2023-10-04T17:28:23,050 [INFO ] W-9000-TinyTransformer_1.0 ACCESS_LOG - /127.0.0.1:56190 "POST /predictions/TinyTransformer HTTP/1.1" 200 32
2023-10-04T17:28:23,050 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_METRICS - HandlerTime.ms:30.0|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,requestID:9ce35cb4-4d60-431b-90a5-a61b1c2dfb4c,timestamp:1696454903
2023-10-04T17:28:23,050 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:30.1|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,1696454903,9ce35cb4-4d60-431b-90a5-a61b1c2dfb4c, pattern=[METRICS]
2023-10-04T17:28:23,050 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:30.1|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,1696454903,9ce35cb4-4d60-431b-90a5-a61b1c2dfb4c, pattern=[METRICS]
2023-10-04T17:28:23,050 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:berserker,timestamp:1696454903
2023-10-04T17:28:23,051 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_METRICS - PredictionTime.ms:30.1|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,requestID:9ce35cb4-4d60-431b-90a5-a61b1c2dfb4c,timestamp:1696454903
2023-10-04T17:28:23,051 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - ts_inference_latency_microseconds.Microseconds:31467.523|#model_name:TinyTransformer,model_version:default|#hostname:berserker,timestamp:1696454903
2023-10-04T17:28:23,051 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - ts_queue_latency_microseconds.Microseconds:47.015|#model_name:TinyTransformer,model_version:default|#hostname:berserker,timestamp:1696454903
2023-10-04T17:28:23,051 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.job.Job - Waiting time ns: 47015, Backend time ns: 32190475
2023-10-04T17:28:23,051 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.job.Job - Waiting time ns: 47015, Backend time ns: 32190475
2023-10-04T17:28:23,051 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - QueueTime.Milliseconds:0.0|#Level:Host|#hostname:berserker,timestamp:1696454903
2023-10-04T17:28:23,051 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - sent a reply, jobdone: true
2023-10-04T17:28:23,051 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - sent a reply, jobdone: true
2023-10-04T17:28:23,051 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 30
2023-10-04T17:28:23,051 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 30
2023-10-04T17:28:23,051 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:2.0|#Level:Host|#hostname:berserker,timestamp:1696454903
2023-10-04T17:28:39,061 [INFO ] epollEventLoopGroup-3-3 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:TinyTransformer,model_version:default|#hostname:berserker,timestamp:1696454919
2023-10-04T17:28:39,062 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1696454919062
2023-10-04T17:28:39,062 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1696454919062
2023-10-04T17:28:39,063 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Backend received inference at: 1696454919
2023-10-04T17:28:39,111 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - model_input KThe dog is running.
2023-10-04T17:28:39,111 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]HandlerTime.Milliseconds:47.61|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,1696454919,75a8a926-bb8f-4017-a80b-11e068dc74e1, pattern=[METRICS]
2023-10-04T17:28:39,111 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]HandlerTime.Milliseconds:47.61|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,1696454919,75a8a926-bb8f-4017-a80b-11e068dc74e1, pattern=[METRICS]
2023-10-04T17:28:39,111 [INFO ] W-9000-TinyTransformer_1.0 ACCESS_LOG - /127.0.0.1:60028 "POST /predictions/TinyTransformer HTTP/1.1" 200 50
2023-10-04T17:28:39,111 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_METRICS - HandlerTime.ms:47.61|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,requestID:75a8a926-bb8f-4017-a80b-11e068dc74e1,timestamp:1696454919
2023-10-04T17:28:39,111 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:berserker,timestamp:1696454919
2023-10-04T17:28:39,111 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:47.75|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,1696454919,75a8a926-bb8f-4017-a80b-11e068dc74e1, pattern=[METRICS]
2023-10-04T17:28:39,111 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:47.75|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,1696454919,75a8a926-bb8f-4017-a80b-11e068dc74e1, pattern=[METRICS]
2023-10-04T17:28:39,112 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - ts_inference_latency_microseconds.Microseconds:49080.354|#model_name:TinyTransformer,model_version:default|#hostname:berserker,timestamp:1696454919
2023-10-04T17:28:39,112 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_METRICS - PredictionTime.ms:47.75|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,requestID:75a8a926-bb8f-4017-a80b-11e068dc74e1,timestamp:1696454919
2023-10-04T17:28:39,112 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - ts_queue_latency_microseconds.Microseconds:74.398|#model_name:TinyTransformer,model_version:default|#hostname:berserker,timestamp:1696454919
2023-10-04T17:28:39,112 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.job.Job - Waiting time ns: 74398, Backend time ns: 50001518
2023-10-04T17:28:39,112 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.job.Job - Waiting time ns: 74398, Backend time ns: 50001518
2023-10-04T17:28:39,112 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - QueueTime.Milliseconds:0.0|#Level:Host|#hostname:berserker,timestamp:1696454919
2023-10-04T17:28:39,112 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - sent a reply, jobdone: true
2023-10-04T17:28:39,112 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - sent a reply, jobdone: true
2023-10-04T17:28:39,112 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 48
2023-10-04T17:28:39,112 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 48
2023-10-04T17:28:39,112 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:2.0|#Level:Host|#hostname:berserker,timestamp:1696454919
2023-10-04T17:28:42,507 [INFO ] epollEventLoopGroup-3-4 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:TinyTransformer,model_version:default|#hostname:berserker,timestamp:1696454922
2023-10-04T17:28:42,508 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1696454922507
2023-10-04T17:28:42,508 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1696454922507
2023-10-04T17:28:42,508 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Backend received inference at: 1696454922
2023-10-04T17:28:42,548 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - model_input KThe dog is running.
2023-10-04T17:28:42,548 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]HandlerTime.Milliseconds:39.66|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,1696454922,063832a2-add3-4178-a17f-1fbb0c19dcda, pattern=[METRICS]
2023-10-04T17:28:42,548 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]HandlerTime.Milliseconds:39.66|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,1696454922,063832a2-add3-4178-a17f-1fbb0c19dcda, pattern=[METRICS]
2023-10-04T17:28:42,548 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_METRICS - HandlerTime.ms:39.66|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,requestID:063832a2-add3-4178-a17f-1fbb0c19dcda,timestamp:1696454922
2023-10-04T17:28:42,548 [INFO ] W-9000-TinyTransformer_1.0 ACCESS_LOG - /127.0.0.1:60038 "POST /predictions/TinyTransformer HTTP/1.1" 200 41
2023-10-04T17:28:42,549 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:berserker,timestamp:1696454922
2023-10-04T17:28:42,549 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:39.78|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,1696454922,063832a2-add3-4178-a17f-1fbb0c19dcda, pattern=[METRICS]
2023-10-04T17:28:42,549 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:39.78|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,1696454922,063832a2-add3-4178-a17f-1fbb0c19dcda, pattern=[METRICS]
2023-10-04T17:28:42,549 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - ts_inference_latency_microseconds.Microseconds:40871.539|#model_name:TinyTransformer,model_version:default|#hostname:berserker,timestamp:1696454922
2023-10-04T17:28:42,549 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - ts_queue_latency_microseconds.Microseconds:54.291|#model_name:TinyTransformer,model_version:default|#hostname:berserker,timestamp:1696454922
2023-10-04T17:28:42,549 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_METRICS - PredictionTime.ms:39.78|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,requestID:063832a2-add3-4178-a17f-1fbb0c19dcda,timestamp:1696454922
2023-10-04T17:28:42,549 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.job.Job - Waiting time ns: 54291, Backend time ns: 41433541
2023-10-04T17:28:42,549 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.job.Job - Waiting time ns: 54291, Backend time ns: 41433541
2023-10-04T17:28:42,549 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - QueueTime.Milliseconds:0.0|#Level:Host|#hostname:berserker,timestamp:1696454922
2023-10-04T17:28:42,549 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - sent a reply, jobdone: true
2023-10-04T17:28:42,549 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - sent a reply, jobdone: true
2023-10-04T17:28:42,549 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 40
2023-10-04T17:28:42,549 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 40
2023-10-04T17:28:42,549 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:2.0|#Level:Host|#hostname:berserker,timestamp:1696454922
2023-10-04T17:29:07,168 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2023-10-04T17:29:07,168 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2023-10-04T17:29:21,338 [INFO ] epollEventLoopGroup-3-5 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:TinyTransformer,model_version:default|#hostname:berserker,timestamp:1696454961
2023-10-04T17:29:21,339 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1696454961339
2023-10-04T17:29:21,339 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1696454961339
2023-10-04T17:29:21,340 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Backend received inference at: 1696454961
2023-10-04T17:29:21,377 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - model_input KThe dog is running.
2023-10-04T17:29:21,377 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]HandlerTime.Milliseconds:37.33|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,1696454961,b4f251b6-667c-46b3-a8bc-9c53d3a73224, pattern=[METRICS]
2023-10-04T17:29:21,377 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]HandlerTime.Milliseconds:37.33|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,1696454961,b4f251b6-667c-46b3-a8bc-9c53d3a73224, pattern=[METRICS]
2023-10-04T17:29:21,378 [INFO ] W-9000-TinyTransformer_1.0 ACCESS_LOG - /127.0.0.1:57010 "POST /predictions/TinyTransformer HTTP/1.1" 200 39
2023-10-04T17:29:21,378 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_METRICS - HandlerTime.ms:37.33|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,requestID:b4f251b6-667c-46b3-a8bc-9c53d3a73224,timestamp:1696454961
2023-10-04T17:29:21,378 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:berserker,timestamp:1696454961
2023-10-04T17:29:21,378 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:37.45|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,1696454961,b4f251b6-667c-46b3-a8bc-9c53d3a73224, pattern=[METRICS]
2023-10-04T17:29:21,378 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:37.45|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,1696454961,b4f251b6-667c-46b3-a8bc-9c53d3a73224, pattern=[METRICS]
2023-10-04T17:29:21,378 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - ts_inference_latency_microseconds.Microseconds:38663.933|#model_name:TinyTransformer,model_version:default|#hostname:berserker,timestamp:1696454961
2023-10-04T17:29:21,378 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_METRICS - PredictionTime.ms:37.45|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,requestID:b4f251b6-667c-46b3-a8bc-9c53d3a73224,timestamp:1696454961
2023-10-04T17:29:21,378 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - ts_queue_latency_microseconds.Microseconds:70.424|#model_name:TinyTransformer,model_version:default|#hostname:berserker,timestamp:1696454961
2023-10-04T17:29:21,378 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.job.Job - Waiting time ns: 70424, Backend time ns: 39363121
2023-10-04T17:29:21,378 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.job.Job - Waiting time ns: 70424, Backend time ns: 39363121
2023-10-04T17:29:21,378 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - QueueTime.Milliseconds:0.0|#Level:Host|#hostname:berserker,timestamp:1696454961
2023-10-04T17:29:21,378 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - sent a reply, jobdone: true
2023-10-04T17:29:21,378 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - sent a reply, jobdone: true
2023-10-04T17:29:21,378 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 38
2023-10-04T17:29:21,378 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 38
2023-10-04T17:29:21,379 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:1.0|#Level:Host|#hostname:berserker,timestamp:1696454961
2023-10-04T17:29:56,000 [INFO ] epollEventLoopGroup-3-6 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:TinyTransformer,model_version:default|#hostname:berserker,timestamp:1696454996
2023-10-04T17:29:56,002 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1696454996001
2023-10-04T17:29:56,002 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1696454996001
2023-10-04T17:29:56,004 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Backend received inference at: 1696454996
2023-10-04T17:29:56,067 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - model_input The dog is running.
2023-10-04T17:29:56,068 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]HandlerTime.Milliseconds:63.43|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,1696454996,0a2fe3b4-7ae4-41c6-a109-301ca2f9e242, pattern=[METRICS]
2023-10-04T17:29:56,068 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]HandlerTime.Milliseconds:63.43|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,1696454996,0a2fe3b4-7ae4-41c6-a109-301ca2f9e242, pattern=[METRICS]
2023-10-04T17:29:56,068 [INFO ] W-9000-TinyTransformer_1.0 ACCESS_LOG - /127.0.0.1:45758 "POST /predictions/TinyTransformer HTTP/1.1" 200 68
2023-10-04T17:29:56,068 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:berserker,timestamp:1696454996
2023-10-04T17:29:56,068 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_METRICS - HandlerTime.ms:63.43|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,requestID:0a2fe3b4-7ae4-41c6-a109-301ca2f9e242,timestamp:1696454996
2023-10-04T17:29:56,068 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - ts_inference_latency_microseconds.Microseconds:66320.727|#model_name:TinyTransformer,model_version:default|#hostname:berserker,timestamp:1696454996
2023-10-04T17:29:56,068 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:63.61|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,1696454996,0a2fe3b4-7ae4-41c6-a109-301ca2f9e242, pattern=[METRICS]
2023-10-04T17:29:56,068 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - ts_queue_latency_microseconds.Microseconds:136.804|#model_name:TinyTransformer,model_version:default|#hostname:berserker,timestamp:1696454996
2023-10-04T17:29:56,068 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:63.61|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,1696454996,0a2fe3b4-7ae4-41c6-a109-301ca2f9e242, pattern=[METRICS]
2023-10-04T17:29:56,068 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.job.Job - Waiting time ns: 136804, Backend time ns: 66936716
2023-10-04T17:29:56,068 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.job.Job - Waiting time ns: 136804, Backend time ns: 66936716
2023-10-04T17:29:56,068 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_METRICS - PredictionTime.ms:63.61|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,requestID:0a2fe3b4-7ae4-41c6-a109-301ca2f9e242,timestamp:1696454996
2023-10-04T17:29:56,068 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - QueueTime.Milliseconds:0.0|#Level:Host|#hostname:berserker,timestamp:1696454996
2023-10-04T17:29:56,069 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - sent a reply, jobdone: true
2023-10-04T17:29:56,069 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - sent a reply, jobdone: true
2023-10-04T17:29:56,069 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 65
2023-10-04T17:29:56,069 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 65
2023-10-04T17:29:56,069 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:3.0|#Level:Host|#hostname:berserker,timestamp:1696454996
2023-10-04T17:30:07,167 [ERROR] Thread-3 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2023-10-04T17:30:07,167 [ERROR] Thread-3 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2023-10-04T17:30:34,389 [INFO ] epollEventLoopGroup-3-7 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:TinyTransformer,model_version:default|#hostname:berserker,timestamp:1696455034
2023-10-04T17:30:34,389 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1696455034389
2023-10-04T17:30:34,389 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1696455034389
2023-10-04T17:30:34,390 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Backend received inference at: 1696455034
2023-10-04T17:30:34,412 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - model_input The dog is running.
2023-10-04T17:30:34,412 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]HandlerTime.Milliseconds:21.47|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,1696455034,252abbe9-6847-488b-b317-08e7dc961624, pattern=[METRICS]
2023-10-04T17:30:34,412 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]HandlerTime.Milliseconds:21.47|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,1696455034,252abbe9-6847-488b-b317-08e7dc961624, pattern=[METRICS]
2023-10-04T17:30:34,412 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_METRICS - HandlerTime.ms:21.47|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,requestID:252abbe9-6847-488b-b317-08e7dc961624,timestamp:1696455034
2023-10-04T17:30:34,412 [INFO ] W-9000-TinyTransformer_1.0 ACCESS_LOG - /127.0.0.1:59432 "POST /predictions/TinyTransformer HTTP/1.1" 200 23
2023-10-04T17:30:34,412 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:21.57|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,1696455034,252abbe9-6847-488b-b317-08e7dc961624, pattern=[METRICS]
2023-10-04T17:30:34,412 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:21.57|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,1696455034,252abbe9-6847-488b-b317-08e7dc961624, pattern=[METRICS]
2023-10-04T17:30:34,412 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_METRICS - PredictionTime.ms:21.57|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,requestID:252abbe9-6847-488b-b317-08e7dc961624,timestamp:1696455034
2023-10-04T17:30:34,412 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:berserker,timestamp:1696455034
2023-10-04T17:30:34,412 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - ts_inference_latency_microseconds.Microseconds:22755.514|#model_name:TinyTransformer,model_version:default|#hostname:berserker,timestamp:1696455034
2023-10-04T17:30:34,412 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - ts_queue_latency_microseconds.Microseconds:187.826|#model_name:TinyTransformer,model_version:default|#hostname:berserker,timestamp:1696455034
2023-10-04T17:30:34,413 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.job.Job - Waiting time ns: 187826, Backend time ns: 23174494
2023-10-04T17:30:34,413 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.job.Job - Waiting time ns: 187826, Backend time ns: 23174494
2023-10-04T17:30:34,413 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - QueueTime.Milliseconds:0.0|#Level:Host|#hostname:berserker,timestamp:1696455034
2023-10-04T17:30:34,413 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - sent a reply, jobdone: true
2023-10-04T17:30:34,413 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - sent a reply, jobdone: true
2023-10-04T17:30:34,413 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 22
2023-10-04T17:30:34,413 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 22
2023-10-04T17:30:34,413 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:2.0|#Level:Host|#hostname:berserker,timestamp:1696455034
2023-10-04T17:30:41,679 [INFO ] epollEventLoopGroup-3-8 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:TinyTransformer,model_version:default|#hostname:berserker,timestamp:1696455041
2023-10-04T17:30:41,679 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1696455041679
2023-10-04T17:30:41,679 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1696455041679
2023-10-04T17:30:41,680 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Backend received inference at: 1696455041
2023-10-04T17:30:41,716 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - model_input The dog is running in a park.
2023-10-04T17:30:41,716 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]HandlerTime.Milliseconds:35.81|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,1696455041,c2d78702-1e8e-44b9-a822-811e4c51f71a, pattern=[METRICS]
2023-10-04T17:30:41,716 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]HandlerTime.Milliseconds:35.81|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,1696455041,c2d78702-1e8e-44b9-a822-811e4c51f71a, pattern=[METRICS]
2023-10-04T17:30:41,716 [INFO ] W-9000-TinyTransformer_1.0 ACCESS_LOG - /127.0.0.1:39614 "POST /predictions/TinyTransformer HTTP/1.1" 200 38
2023-10-04T17:30:41,716 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_METRICS - HandlerTime.ms:35.81|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,requestID:c2d78702-1e8e-44b9-a822-811e4c51f71a,timestamp:1696455041
2023-10-04T17:30:41,716 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:35.93|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,1696455041,c2d78702-1e8e-44b9-a822-811e4c51f71a, pattern=[METRICS]
2023-10-04T17:30:41,716 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:berserker,timestamp:1696455041
2023-10-04T17:30:41,716 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:35.93|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,1696455041,c2d78702-1e8e-44b9-a822-811e4c51f71a, pattern=[METRICS]
2023-10-04T17:30:41,716 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_METRICS - PredictionTime.ms:35.93|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,requestID:c2d78702-1e8e-44b9-a822-811e4c51f71a,timestamp:1696455041
2023-10-04T17:30:41,716 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - ts_inference_latency_microseconds.Microseconds:37011.036|#model_name:TinyTransformer,model_version:default|#hostname:berserker,timestamp:1696455041
2023-10-04T17:30:41,717 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - ts_queue_latency_microseconds.Microseconds:189.26|#model_name:TinyTransformer,model_version:default|#hostname:berserker,timestamp:1696455041
2023-10-04T17:30:41,717 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.job.Job - Waiting time ns: 189260, Backend time ns: 37636852
2023-10-04T17:30:41,717 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.job.Job - Waiting time ns: 189260, Backend time ns: 37636852
2023-10-04T17:30:41,717 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - QueueTime.Milliseconds:0.0|#Level:Host|#hostname:berserker,timestamp:1696455041
2023-10-04T17:30:41,717 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - sent a reply, jobdone: true
2023-10-04T17:30:41,717 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - sent a reply, jobdone: true
2023-10-04T17:30:41,717 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 37
2023-10-04T17:30:41,717 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 37
2023-10-04T17:30:41,717 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:1.0|#Level:Host|#hostname:berserker,timestamp:1696455041
2023-10-04T17:30:45,918 [INFO ] epollEventLoopGroup-3-9 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:TinyTransformer,model_version:default|#hostname:berserker,timestamp:1696455045
2023-10-04T17:30:45,918 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1696455045918
2023-10-04T17:30:45,918 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1696455045918
2023-10-04T17:30:45,919 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Backend received inference at: 1696455045
2023-10-04T17:30:45,957 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - model_input The dog is running in a park.
2023-10-04T17:30:45,957 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]HandlerTime.Milliseconds:37.63|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,1696455045,fbad4178-4bf3-454e-a925-bc0cf4d66451, pattern=[METRICS]
2023-10-04T17:30:45,957 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]HandlerTime.Milliseconds:37.63|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,1696455045,fbad4178-4bf3-454e-a925-bc0cf4d66451, pattern=[METRICS]
2023-10-04T17:30:45,957 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_METRICS - HandlerTime.ms:37.63|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,requestID:fbad4178-4bf3-454e-a925-bc0cf4d66451,timestamp:1696455045
2023-10-04T17:30:45,957 [INFO ] W-9000-TinyTransformer_1.0 ACCESS_LOG - /127.0.0.1:48254 "POST /predictions/TinyTransformer HTTP/1.1" 200 39
2023-10-04T17:30:45,957 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:berserker,timestamp:1696455045
2023-10-04T17:30:45,957 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:37.74|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,1696455045,fbad4178-4bf3-454e-a925-bc0cf4d66451, pattern=[METRICS]
2023-10-04T17:30:45,957 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:37.74|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,1696455045,fbad4178-4bf3-454e-a925-bc0cf4d66451, pattern=[METRICS]
2023-10-04T17:30:45,957 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_METRICS - PredictionTime.ms:37.74|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,requestID:fbad4178-4bf3-454e-a925-bc0cf4d66451,timestamp:1696455045
2023-10-04T17:30:45,957 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - ts_inference_latency_microseconds.Microseconds:38706.895|#model_name:TinyTransformer,model_version:default|#hostname:berserker,timestamp:1696455045
2023-10-04T17:30:45,958 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - ts_queue_latency_microseconds.Microseconds:51.516|#model_name:TinyTransformer,model_version:default|#hostname:berserker,timestamp:1696455045
2023-10-04T17:30:45,958 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.job.Job - Waiting time ns: 51516, Backend time ns: 39401383
2023-10-04T17:30:45,958 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.job.Job - Waiting time ns: 51516, Backend time ns: 39401383
2023-10-04T17:30:45,958 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - QueueTime.Milliseconds:0.0|#Level:Host|#hostname:berserker,timestamp:1696455045
2023-10-04T17:30:45,958 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - sent a reply, jobdone: true
2023-10-04T17:30:45,958 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - sent a reply, jobdone: true
2023-10-04T17:30:45,958 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 38
2023-10-04T17:30:45,958 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 38
2023-10-04T17:30:45,958 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:2.0|#Level:Host|#hostname:berserker,timestamp:1696455045
2023-10-04T17:30:46,943 [INFO ] epollEventLoopGroup-3-10 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:TinyTransformer,model_version:default|#hostname:berserker,timestamp:1696455046
2023-10-04T17:30:46,944 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1696455046944
2023-10-04T17:30:46,944 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1696455046944
2023-10-04T17:30:46,945 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Backend received inference at: 1696455046
2023-10-04T17:30:46,988 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - model_input The dog is running in a park.
2023-10-04T17:30:46,988 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]HandlerTime.Milliseconds:42.44|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,1696455046,542cf0bd-e76a-47e1-b9fb-37393996876e, pattern=[METRICS]
2023-10-04T17:30:46,988 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]HandlerTime.Milliseconds:42.44|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,1696455046,542cf0bd-e76a-47e1-b9fb-37393996876e, pattern=[METRICS]
2023-10-04T17:30:46,988 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_METRICS - HandlerTime.ms:42.44|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,requestID:542cf0bd-e76a-47e1-b9fb-37393996876e,timestamp:1696455046
2023-10-04T17:30:46,988 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:42.55|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,1696455046,542cf0bd-e76a-47e1-b9fb-37393996876e, pattern=[METRICS]
2023-10-04T17:30:46,988 [INFO ] W-9000-TinyTransformer_1.0 ACCESS_LOG - /127.0.0.1:48268 "POST /predictions/TinyTransformer HTTP/1.1" 200 46
2023-10-04T17:30:46,988 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:42.55|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,1696455046,542cf0bd-e76a-47e1-b9fb-37393996876e, pattern=[METRICS]
2023-10-04T17:30:46,988 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:berserker,timestamp:1696455046
2023-10-04T17:30:46,988 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_METRICS - PredictionTime.ms:42.55|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,requestID:542cf0bd-e76a-47e1-b9fb-37393996876e,timestamp:1696455046
2023-10-04T17:30:46,988 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - ts_inference_latency_microseconds.Microseconds:44231.747|#model_name:TinyTransformer,model_version:default|#hostname:berserker,timestamp:1696455046
2023-10-04T17:30:46,988 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - ts_queue_latency_microseconds.Microseconds:159.369|#model_name:TinyTransformer,model_version:default|#hostname:berserker,timestamp:1696455046
2023-10-04T17:30:46,988 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.job.Job - Waiting time ns: 159369, Backend time ns: 44704182
2023-10-04T17:30:46,988 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.job.Job - Waiting time ns: 159369, Backend time ns: 44704182
2023-10-04T17:30:46,988 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - QueueTime.Milliseconds:0.0|#Level:Host|#hostname:berserker,timestamp:1696455046
2023-10-04T17:30:46,989 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - sent a reply, jobdone: true
2023-10-04T17:30:46,989 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - sent a reply, jobdone: true
2023-10-04T17:30:46,989 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 44
2023-10-04T17:30:46,989 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 44
2023-10-04T17:30:46,989 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:1.0|#Level:Host|#hostname:berserker,timestamp:1696455046
2023-10-04T17:30:47,873 [INFO ] epollEventLoopGroup-3-11 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:TinyTransformer,model_version:default|#hostname:berserker,timestamp:1696455047
2023-10-04T17:30:47,873 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1696455047873
2023-10-04T17:30:47,873 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1696455047873
2023-10-04T17:30:47,874 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Backend received inference at: 1696455047
2023-10-04T17:30:47,912 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - model_input The dog is running in a park.
2023-10-04T17:30:47,912 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]HandlerTime.Milliseconds:37.3|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,1696455047,5fc5bec3-3a71-4562-b437-3be4d1ee4810, pattern=[METRICS]
2023-10-04T17:30:47,912 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]HandlerTime.Milliseconds:37.3|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,1696455047,5fc5bec3-3a71-4562-b437-3be4d1ee4810, pattern=[METRICS]
2023-10-04T17:30:47,912 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_METRICS - HandlerTime.ms:37.3|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,requestID:5fc5bec3-3a71-4562-b437-3be4d1ee4810,timestamp:1696455047
2023-10-04T17:30:47,912 [INFO ] W-9000-TinyTransformer_1.0 ACCESS_LOG - /127.0.0.1:48270 "POST /predictions/TinyTransformer HTTP/1.1" 200 39
2023-10-04T17:30:47,920 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:37.4|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,1696455047,5fc5bec3-3a71-4562-b437-3be4d1ee4810, pattern=[METRICS]
2023-10-04T17:30:47,920 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:berserker,timestamp:1696455047
2023-10-04T17:30:47,920 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:37.4|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,1696455047,5fc5bec3-3a71-4562-b437-3be4d1ee4810, pattern=[METRICS]
2023-10-04T17:30:47,920 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - ts_inference_latency_microseconds.Microseconds:38610.563|#model_name:TinyTransformer,model_version:default|#hostname:berserker,timestamp:1696455047
2023-10-04T17:30:47,920 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_METRICS - PredictionTime.ms:37.4|#ModelName:TinyTransformer,Level:Model|#hostname:berserker,requestID:5fc5bec3-3a71-4562-b437-3be4d1ee4810,timestamp:1696455047
2023-10-04T17:30:47,920 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - ts_queue_latency_microseconds.Microseconds:100.909|#model_name:TinyTransformer,model_version:default|#hostname:berserker,timestamp:1696455047
2023-10-04T17:30:47,920 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.job.Job - Waiting time ns: 100909, Backend time ns: 46914850
2023-10-04T17:30:47,920 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.job.Job - Waiting time ns: 100909, Backend time ns: 46914850
2023-10-04T17:30:47,920 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - QueueTime.Milliseconds:0.0|#Level:Host|#hostname:berserker,timestamp:1696455047
2023-10-04T17:30:47,920 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - sent a reply, jobdone: true
2023-10-04T17:30:47,920 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - sent a reply, jobdone: true
2023-10-04T17:30:47,920 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 38
2023-10-04T17:30:47,920 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 38
2023-10-04T17:30:47,920 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:9.0|#Level:Host|#hostname:berserker,timestamp:1696455047
2023-10-04T17:31:07,171 [ERROR] Thread-4 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2023-10-04T17:31:07,171 [ERROR] Thread-4 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

