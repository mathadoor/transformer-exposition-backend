2023-10-04T14:38:23,627 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2023-10-04T14:38:23,627 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2023-10-04T14:38:23,652 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2023-10-04T14:38:23,652 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2023-10-04T14:38:23,653 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2023-10-04T14:38:23,653 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2023-10-04T14:38:23,679 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml
2023-10-04T14:38:23,679 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml
2023-10-04T14:38:23,775 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.8.2
TS Home: /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages
Current directory: /home/mathadoor/Documents/transformer-exposition-backend/app
Temp directory: /tmp
Metrics config path: /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 32
Max heap size: 16000 M
Python executable: /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/bin/python3.9
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/mathadoor/Documents/transformer-exposition-backend/app
Initial Models: TinyTransformer.mar
Log dir: /home/mathadoor/Documents/transformer-exposition-backend/app/logs
Metrics dir: /home/mathadoor/Documents/transformer-exposition-backend/app/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: log
Disable system metrics: false
Workflow Store: /home/mathadoor/Documents/transformer-exposition-backend/app
Model config: N/A
2023-10-04T14:38:23,775 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.8.2
TS Home: /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages
Current directory: /home/mathadoor/Documents/transformer-exposition-backend/app
Temp directory: /tmp
Metrics config path: /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 32
Max heap size: 16000 M
Python executable: /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/bin/python3.9
Config file: N/A
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/mathadoor/Documents/transformer-exposition-backend/app
Initial Models: TinyTransformer.mar
Log dir: /home/mathadoor/Documents/transformer-exposition-backend/app/logs
Metrics dir: /home/mathadoor/Documents/transformer-exposition-backend/app/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: log
Disable system metrics: false
Workflow Store: /home/mathadoor/Documents/transformer-exposition-backend/app
Model config: N/A
2023-10-04T14:38:23,782 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: TinyTransformer.mar
2023-10-04T14:38:23,782 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: TinyTransformer.mar
2023-10-04T14:38:23,943 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model TinyTransformer
2023-10-04T14:38:23,943 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model TinyTransformer
2023-10-04T14:38:23,943 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model TinyTransformer
2023-10-04T14:38:23,943 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model TinyTransformer
2023-10-04T14:38:23,943 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model TinyTransformer loaded.
2023-10-04T14:38:23,943 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model TinyTransformer loaded.
2023-10-04T14:38:23,943 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: TinyTransformer, count: 1
2023-10-04T14:38:23,943 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: TinyTransformer, count: 1
2023-10-04T14:38:23,948 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/bin/python3.9, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml]
2023-10-04T14:38:23,948 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2023-10-04T14:38:23,948 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2023-10-04T14:38:23,948 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/bin/python3.9, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml]
2023-10-04T14:38:23,981 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2023-10-04T14:38:23,981 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2023-10-04T14:38:23,981 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2023-10-04T14:38:23,981 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2023-10-04T14:38:23,981 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2023-10-04T14:38:23,981 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2023-10-04T14:38:23,982 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2023-10-04T14:38:23,982 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2023-10-04T14:38:23,982 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2023-10-04T14:38:23,982 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2023-10-04T14:38:24,088 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2023-10-04T14:38:24,088 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2023-10-04T14:38:24,122 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2023-10-04T14:38:24,122 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2023-10-04T14:38:24,650 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=19499
2023-10-04T14:38:24,650 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-10-04T14:38:24,679 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Successfully loaded /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml.
2023-10-04T14:38:24,679 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - [PID]19499
2023-10-04T14:38:24,679 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Torch worker started.
2023-10-04T14:38:24,679 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Python runtime: 3.9.18
2023-10-04T14:38:24,679 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change null -> WORKER_STARTED
2023-10-04T14:38:24,679 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change null -> WORKER_STARTED
2023-10-04T14:38:24,682 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-10-04T14:38:24,682 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-10-04T14:38:24,687 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2023-10-04T14:38:24,688 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD to backend at: 1696444704688
2023-10-04T14:38:24,688 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD to backend at: 1696444704688
2023-10-04T14:38:24,709 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - model_name: TinyTransformer, batchSize: 1
2023-10-04T14:38:24,735 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - proceeding without onnxruntime
2023-10-04T14:38:24,735 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2023-10-04T14:38:24,735 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Backend worker process died.
2023-10-04T14:38:24,735 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2023-10-04T14:38:24,735 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG -   File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/model_service_worker.py", line 253, in <module>
2023-10-04T14:38:24,735 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG -     worker.run_server()
2023-10-04T14:38:24,736 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG -   File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/model_service_worker.py", line 221, in run_server
2023-10-04T14:38:24,736 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2023-10-04T14:38:24,736 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-10-04T14:38:24,736 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG -   File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/model_service_worker.py", line 184, in handle_connection
2023-10-04T14:38:24,736 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-10-04T14:38:24,736 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2023-10-04T14:38:24,736 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG -   File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/model_service_worker.py", line 131, in load_model
2023-10-04T14:38:24,736 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-10-04T14:38:24,736 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG -     service = model_loader.load(
2023-10-04T14:38:24,736 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-10-04T14:38:24,736 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG -   File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/model_loader.py", line 135, in load
2023-10-04T14:38:24,736 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2023-10-04T14:38:24,737 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG -   File "/tmp/models/b94cb400ccc4486aab069672a968187c/TranslationHandler.py", line 32, in initialize
2023-10-04T14:38:24,737 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG -     with open('english_tokenizer.pkl', 'rb') as f:
2023-10-04T14:38:24,737 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - FileNotFoundError: [Errno 2] No such file or directory: 'english_tokenizer.pkl'
2023-10-04T14:38:24,736 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:213) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2023-10-04T14:38:24,736 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:213) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2023-10-04T14:38:24,742 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: TinyTransformer, error: Worker died.
2023-10-04T14:38:24,742 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: TinyTransformer, error: Worker died.
2023-10-04T14:38:24,742 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-10-04T14:38:24,742 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-10-04T14:38:24,742 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1696444704742
2023-10-04T14:38:24,742 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1696444704742
2023-10-04T14:38:24,742 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-TinyTransformer_1.0-stderr
2023-10-04T14:38:24,742 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-TinyTransformer_1.0-stderr
2023-10-04T14:38:24,742 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-TinyTransformer_1.0-stdout
2023-10-04T14:38:24,742 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-TinyTransformer_1.0-stdout
2023-10-04T14:38:24,742 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2023-10-04T14:38:24,742 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2023-10-04T14:38:24,747 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-TinyTransformer_1.0-stdout
2023-10-04T14:38:24,747 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-TinyTransformer_1.0-stdout
2023-10-04T14:38:24,747 [INFO ] W-9000-TinyTransformer_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-TinyTransformer_1.0-stderr
2023-10-04T14:38:24,747 [INFO ] W-9000-TinyTransformer_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-TinyTransformer_1.0-stderr
2023-10-04T14:38:25,744 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/bin/python3.9, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml]
2023-10-04T14:38:25,744 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/bin/python3.9, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml]
2023-10-04T14:38:26,455 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=19569
2023-10-04T14:38:26,456 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-10-04T14:38:26,480 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Successfully loaded /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml.
2023-10-04T14:38:26,481 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - [PID]19569
2023-10-04T14:38:26,481 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Torch worker started.
2023-10-04T14:38:26,481 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Python runtime: 3.9.18
2023-10-04T14:38:26,481 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-10-04T14:38:26,481 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-10-04T14:38:26,481 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-10-04T14:38:26,481 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-10-04T14:38:26,482 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-10-04T14:38:26,482 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-10-04T14:38:26,482 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2023-10-04T14:38:26,482 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-10-04T14:38:26,482 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-10-04T14:38:26,482 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.pollFirst(LinkedBlockingDeque.java:515) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.poll(LinkedBlockingDeque.java:677) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:276) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:34) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:186) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2023-10-04T14:38:26,482 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.pollFirst(LinkedBlockingDeque.java:515) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.poll(LinkedBlockingDeque.java:677) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:276) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:34) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:186) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2023-10-04T14:38:26,483 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-10-04T14:38:26,483 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-10-04T14:38:26,483 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-10-04T14:38:26,483 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-10-04T14:38:26,483 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-TinyTransformer_1.0-stderr
2023-10-04T14:38:26,483 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-TinyTransformer_1.0-stderr
2023-10-04T14:38:26,483 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-TinyTransformer_1.0-stdout
2023-10-04T14:38:26,483 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-TinyTransformer_1.0-stdout
2023-10-04T14:38:26,483 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2023-10-04T14:38:26,483 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2023-10-04T14:38:26,489 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-TinyTransformer_1.0-stdout
2023-10-04T14:38:26,489 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-TinyTransformer_1.0-stdout
2023-10-04T14:38:26,489 [INFO ] W-9000-TinyTransformer_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-TinyTransformer_1.0-stderr
2023-10-04T14:38:26,489 [INFO ] W-9000-TinyTransformer_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-TinyTransformer_1.0-stderr
2023-10-04T14:38:27,484 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/bin/python3.9, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml]
2023-10-04T14:38:27,484 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/bin/python3.9, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml]
2023-10-04T14:38:28,192 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=19605
2023-10-04T14:38:28,192 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-10-04T14:38:28,225 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Successfully loaded /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml.
2023-10-04T14:38:28,225 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - [PID]19605
2023-10-04T14:38:28,225 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Torch worker started.
2023-10-04T14:38:28,225 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-10-04T14:38:28,225 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Python runtime: 3.9.18
2023-10-04T14:38:28,225 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-10-04T14:38:28,226 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-10-04T14:38:28,226 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-10-04T14:38:28,226 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-10-04T14:38:28,226 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-10-04T14:38:28,226 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2023-10-04T14:38:28,227 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-10-04T14:38:28,227 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-10-04T14:38:28,227 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.pollFirst(LinkedBlockingDeque.java:515) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.poll(LinkedBlockingDeque.java:677) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:276) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:34) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:186) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2023-10-04T14:38:28,227 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.pollFirst(LinkedBlockingDeque.java:515) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.poll(LinkedBlockingDeque.java:677) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:276) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:34) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:186) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2023-10-04T14:38:28,227 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-10-04T14:38:28,227 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-10-04T14:38:28,227 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-10-04T14:38:28,227 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-10-04T14:38:28,227 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-TinyTransformer_1.0-stderr
2023-10-04T14:38:28,227 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-TinyTransformer_1.0-stderr
2023-10-04T14:38:28,227 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-TinyTransformer_1.0-stdout
2023-10-04T14:38:28,227 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-TinyTransformer_1.0-stdout
2023-10-04T14:38:28,227 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2023-10-04T14:38:28,227 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2023-10-04T14:38:28,234 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-TinyTransformer_1.0-stdout
2023-10-04T14:38:28,234 [INFO ] W-9000-TinyTransformer_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-TinyTransformer_1.0-stderr
2023-10-04T14:38:28,234 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-TinyTransformer_1.0-stdout
2023-10-04T14:38:28,234 [INFO ] W-9000-TinyTransformer_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-TinyTransformer_1.0-stderr
2023-10-04T14:45:07,204 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2023-10-04T14:45:07,204 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2023-10-04T14:45:07,231 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2023-10-04T14:45:07,231 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2023-10-04T14:45:07,231 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2023-10-04T14:45:07,231 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2023-10-04T14:45:07,257 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml
2023-10-04T14:45:07,257 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml
2023-10-04T14:45:07,375 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.8.2
TS Home: /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages
Current directory: /home/mathadoor/Documents/transformer-exposition-backend/app
Temp directory: /tmp
Metrics config path: /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 32
Max heap size: 16000 M
Python executable: /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/bin/python3.9
Config file: logs/config/20231004143829399-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/mathadoor/Documents/transformer-exposition-backend/app
Initial Models: TinyTransformer.mar
Log dir: /home/mathadoor/Documents/transformer-exposition-backend/app/logs
Metrics dir: /home/mathadoor/Documents/transformer-exposition-backend/app/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: log
Disable system metrics: false
Workflow Store: /home/mathadoor/Documents/transformer-exposition-backend/app
Model config: N/A
2023-10-04T14:45:07,375 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.8.2
TS Home: /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages
Current directory: /home/mathadoor/Documents/transformer-exposition-backend/app
Temp directory: /tmp
Metrics config path: /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 32
Max heap size: 16000 M
Python executable: /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/bin/python3.9
Config file: logs/config/20231004143829399-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/mathadoor/Documents/transformer-exposition-backend/app
Initial Models: TinyTransformer.mar
Log dir: /home/mathadoor/Documents/transformer-exposition-backend/app/logs
Metrics dir: /home/mathadoor/Documents/transformer-exposition-backend/app/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: log
Disable system metrics: false
Workflow Store: /home/mathadoor/Documents/transformer-exposition-backend/app
Model config: N/A
2023-10-04T14:45:07,380 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20231004143829399-shutdown.cfg",
  "modelCount": 1,
  "created": 1696444709399,
  "models": {
    "TinyTransformer": {
      "1.0": {
        "defaultVersion": true,
        "marName": "TinyTransformer.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2023-10-04T14:45:07,380 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20231004143829399-shutdown.cfg",
  "modelCount": 1,
  "created": 1696444709399,
  "models": {
    "TinyTransformer": {
      "1.0": {
        "defaultVersion": true,
        "marName": "TinyTransformer.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2023-10-04T14:45:07,384 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20231004143829399-shutdown.cfg
2023-10-04T14:45:07,384 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20231004143829399-shutdown.cfg
2023-10-04T14:45:07,385 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20231004143829399-shutdown.cfg validated successfully
2023-10-04T14:45:07,385 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20231004143829399-shutdown.cfg validated successfully
2023-10-04T14:45:07,541 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model TinyTransformer
2023-10-04T14:45:07,541 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model TinyTransformer
2023-10-04T14:45:07,541 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model TinyTransformer
2023-10-04T14:45:07,541 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model TinyTransformer
2023-10-04T14:45:07,541 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model TinyTransformer
2023-10-04T14:45:07,541 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model TinyTransformer
2023-10-04T14:45:07,541 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model TinyTransformer loaded.
2023-10-04T14:45:07,541 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model TinyTransformer loaded.
2023-10-04T14:45:07,541 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: TinyTransformer, count: 1
2023-10-04T14:45:07,541 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: TinyTransformer, count: 1
2023-10-04T14:45:07,546 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2023-10-04T14:45:07,546 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2023-10-04T14:45:07,546 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/bin/python3.9, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml]
2023-10-04T14:45:07,546 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/bin/python3.9, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml]
2023-10-04T14:45:07,579 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2023-10-04T14:45:07,579 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2023-10-04T14:45:07,579 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2023-10-04T14:45:07,579 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2023-10-04T14:45:07,580 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2023-10-04T14:45:07,580 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2023-10-04T14:45:07,580 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2023-10-04T14:45:07,580 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2023-10-04T14:45:07,580 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2023-10-04T14:45:07,580 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2023-10-04T14:45:07,704 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2023-10-04T14:45:07,704 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2023-10-04T14:45:07,738 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2023-10-04T14:45:07,738 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2023-10-04T14:45:08,248 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=20935
2023-10-04T14:45:08,249 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-10-04T14:45:08,275 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Successfully loaded /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml.
2023-10-04T14:45:08,275 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - [PID]20935
2023-10-04T14:45:08,275 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Torch worker started.
2023-10-04T14:45:08,275 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Python runtime: 3.9.18
2023-10-04T14:45:08,275 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change null -> WORKER_STARTED
2023-10-04T14:45:08,275 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change null -> WORKER_STARTED
2023-10-04T14:45:08,278 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-10-04T14:45:08,278 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-10-04T14:45:08,284 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2023-10-04T14:45:08,287 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD to backend at: 1696445108287
2023-10-04T14:45:08,287 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD to backend at: 1696445108287
2023-10-04T14:45:08,302 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - model_name: TinyTransformer, batchSize: 1
2023-10-04T14:45:08,331 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - proceeding without onnxruntime
2023-10-04T14:45:08,331 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2023-10-04T14:45:08,341 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Failed to load model TinyTransformer, exception PytorchStreamReader failed locating file constants.pkl: file not found
2023-10-04T14:45:08,342 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2023-10-04T14:45:08,342 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG -   File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/model_service_worker.py", line 131, in load_model
2023-10-04T14:45:08,342 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG -     service = model_loader.load(
2023-10-04T14:45:08,342 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG -   File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/model_loader.py", line 135, in load
2023-10-04T14:45:08,342 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2023-10-04T14:45:08,342 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG -   File "/tmp/models/feea1b6c37404ebdb46fac57cc16e7e4/TranslationHandler.py", line 60, in initialize
2023-10-04T14:45:08,342 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG -     self.model = torch.jit.load(model_pt_path)
2023-10-04T14:45:08,342 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG -   File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/torch/jit/_serialization.py", line 162, in load
2023-10-04T14:45:08,342 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG -     cpp_module = torch._C.import_ir_module(cu, str(f), map_location, _extra_files, _restore_shapes)  # type: ignore[call-arg]
2023-10-04T14:45:08,342 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - RuntimeError: PytorchStreamReader failed locating file constants.pkl: file not found
2023-10-04T14:45:08,342 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Backend worker process died.
2023-10-04T14:45:08,342 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2023-10-04T14:45:08,343 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG -   File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/model_service_worker.py", line 253, in <module>
2023-10-04T14:45:08,343 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG -     worker.run_server()
2023-10-04T14:45:08,343 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG -   File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/model_service_worker.py", line 221, in run_server
2023-10-04T14:45:08,343 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2023-10-04T14:45:08,343 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG -   File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/model_service_worker.py", line 189, in handle_connection
2023-10-04T14:45:08,343 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG -     raise RuntimeError("{} - {}".format(code, result))
2023-10-04T14:45:08,343 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - RuntimeError: 500 - Unknown exception
2023-10-04T14:45:08,344 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - sent a reply, jobdone: true
2023-10-04T14:45:08,344 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - sent a reply, jobdone: true
2023-10-04T14:45:08,344 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 42
2023-10-04T14:45:08,344 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 42
2023-10-04T14:45:08,344 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change WORKER_STARTED -> WORKER_ERROR
2023-10-04T14:45:08,344 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-10-04T14:45:08,344 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change WORKER_STARTED -> WORKER_ERROR
2023-10-04T14:45:08,344 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-10-04T14:45:08,344 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:15.0|#Level:Host|#hostname:berserker,timestamp:1696445108
2023-10-04T14:45:08,345 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_ERROR
2023-10-04T14:45:08,345 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_ERROR
2023-10-04T14:45:08,345 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.ReentrantLock$Sync.lockInterruptibly(ReentrantLock.java:159) ~[?:?]
	at java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:372) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:272) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:34) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:186) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2023-10-04T14:45:08,345 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.ReentrantLock$Sync.lockInterruptibly(ReentrantLock.java:159) ~[?:?]
	at java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:372) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:272) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:34) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:186) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2023-10-04T14:45:08,351 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change WORKER_ERROR -> WORKER_STOPPED
2023-10-04T14:45:08,351 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change WORKER_ERROR -> WORKER_STOPPED
2023-10-04T14:45:08,351 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1696445108351
2023-10-04T14:45:08,351 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1696445108351
2023-10-04T14:45:08,352 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-TinyTransformer_1.0-stderr
2023-10-04T14:45:08,352 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-TinyTransformer_1.0-stderr
2023-10-04T14:45:08,352 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-TinyTransformer_1.0-stdout
2023-10-04T14:45:08,352 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-TinyTransformer_1.0-stdout
2023-10-04T14:45:08,352 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2023-10-04T14:45:08,352 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2023-10-04T14:45:08,358 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-TinyTransformer_1.0-stdout
2023-10-04T14:45:08,358 [INFO ] W-9000-TinyTransformer_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-TinyTransformer_1.0-stderr
2023-10-04T14:45:08,358 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-TinyTransformer_1.0-stdout
2023-10-04T14:45:08,358 [INFO ] W-9000-TinyTransformer_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-TinyTransformer_1.0-stderr
2023-10-04T14:45:09,353 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/bin/python3.9, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml]
2023-10-04T14:45:09,353 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/bin/python3.9, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml]
2023-10-04T14:45:10,067 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=21005
2023-10-04T14:45:10,068 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-10-04T14:45:10,094 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Successfully loaded /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml.
2023-10-04T14:45:10,094 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - [PID]21005
2023-10-04T14:45:10,094 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Torch worker started.
2023-10-04T14:45:10,094 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Python runtime: 3.9.18
2023-10-04T14:45:10,094 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-10-04T14:45:10,094 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-10-04T14:45:10,095 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-10-04T14:45:10,095 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-10-04T14:45:10,096 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-10-04T14:45:10,096 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-10-04T14:45:10,096 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2023-10-04T14:45:10,096 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-10-04T14:45:10,096 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-10-04T14:45:10,096 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.pollFirst(LinkedBlockingDeque.java:515) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.poll(LinkedBlockingDeque.java:677) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:276) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:34) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:186) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2023-10-04T14:45:10,096 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.pollFirst(LinkedBlockingDeque.java:515) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.poll(LinkedBlockingDeque.java:677) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:276) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:34) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:186) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2023-10-04T14:45:10,096 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-10-04T14:45:10,096 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-10-04T14:45:10,096 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-10-04T14:45:10,096 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-10-04T14:45:10,096 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-TinyTransformer_1.0-stderr
2023-10-04T14:45:10,096 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-TinyTransformer_1.0-stderr
2023-10-04T14:45:10,097 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-TinyTransformer_1.0-stdout
2023-10-04T14:45:10,097 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-TinyTransformer_1.0-stdout
2023-10-04T14:45:10,097 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2023-10-04T14:45:10,097 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2023-10-04T14:45:10,103 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-TinyTransformer_1.0-stdout
2023-10-04T14:45:10,103 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-TinyTransformer_1.0-stdout
2023-10-04T14:45:10,103 [INFO ] W-9000-TinyTransformer_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-TinyTransformer_1.0-stderr
2023-10-04T14:45:10,103 [INFO ] W-9000-TinyTransformer_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-TinyTransformer_1.0-stderr
2023-10-04T14:45:11,098 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/bin/python3.9, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml]
2023-10-04T14:45:11,098 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/bin/python3.9, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml]
2023-10-04T14:45:11,811 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=21040
2023-10-04T14:45:11,811 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-10-04T14:45:11,837 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Successfully loaded /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml.
2023-10-04T14:45:11,837 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - [PID]21040
2023-10-04T14:45:11,837 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Torch worker started.
2023-10-04T14:45:11,837 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Python runtime: 3.9.18
2023-10-04T14:45:11,837 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-10-04T14:45:11,837 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-10-04T14:45:11,837 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-10-04T14:45:11,837 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-10-04T14:45:11,838 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2023-10-04T14:45:11,838 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-10-04T14:45:11,838 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-10-04T14:45:11,839 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-10-04T14:45:11,839 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-10-04T14:45:11,839 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.pollFirst(LinkedBlockingDeque.java:515) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.poll(LinkedBlockingDeque.java:677) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:276) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:34) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:186) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2023-10-04T14:45:11,839 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.pollFirst(LinkedBlockingDeque.java:515) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.poll(LinkedBlockingDeque.java:677) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:276) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:34) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:186) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2023-10-04T14:45:11,839 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-10-04T14:45:11,839 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-10-04T14:45:11,839 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-10-04T14:45:11,839 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-10-04T14:45:11,839 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-TinyTransformer_1.0-stderr
2023-10-04T14:45:11,839 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-TinyTransformer_1.0-stderr
2023-10-04T14:45:11,839 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-TinyTransformer_1.0-stdout
2023-10-04T14:45:11,839 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-TinyTransformer_1.0-stdout
2023-10-04T14:45:11,839 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2023-10-04T14:45:11,839 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2023-10-04T14:45:11,845 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-TinyTransformer_1.0-stdout
2023-10-04T14:45:11,845 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-TinyTransformer_1.0-stdout
2023-10-04T14:45:11,845 [INFO ] W-9000-TinyTransformer_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-TinyTransformer_1.0-stderr
2023-10-04T14:45:11,845 [INFO ] W-9000-TinyTransformer_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-TinyTransformer_1.0-stderr
2023-10-04T14:45:13,840 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/bin/python3.9, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml]
2023-10-04T14:45:13,840 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/bin/python3.9, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml]
2023-10-04T14:45:14,551 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=21084
2023-10-04T14:45:14,551 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-10-04T14:45:14,577 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Successfully loaded /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml.
2023-10-04T14:45:14,577 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - [PID]21084
2023-10-04T14:45:14,577 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Torch worker started.
2023-10-04T14:45:14,577 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Python runtime: 3.9.18
2023-10-04T14:45:14,577 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-10-04T14:45:14,577 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-10-04T14:45:14,578 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-10-04T14:45:14,578 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-10-04T14:45:14,579 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-10-04T14:45:14,579 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-10-04T14:45:14,579 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2023-10-04T14:45:14,579 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-10-04T14:45:14,579 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-10-04T14:45:14,579 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.pollFirst(LinkedBlockingDeque.java:515) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.poll(LinkedBlockingDeque.java:677) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:276) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:34) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:186) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2023-10-04T14:45:14,579 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.pollFirst(LinkedBlockingDeque.java:515) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.poll(LinkedBlockingDeque.java:677) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:276) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:34) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:186) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2023-10-04T14:45:14,579 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-10-04T14:45:14,579 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-10-04T14:45:14,579 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-10-04T14:45:14,579 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-10-04T14:45:14,579 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-TinyTransformer_1.0-stderr
2023-10-04T14:45:14,579 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-TinyTransformer_1.0-stderr
2023-10-04T14:45:14,579 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-TinyTransformer_1.0-stdout
2023-10-04T14:45:14,579 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-TinyTransformer_1.0-stdout
2023-10-04T14:45:14,579 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2023-10-04T14:45:14,579 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.
2023-10-04T14:45:14,586 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-TinyTransformer_1.0-stdout
2023-10-04T14:45:14,586 [INFO ] W-9000-TinyTransformer_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-TinyTransformer_1.0-stderr
2023-10-04T14:45:14,586 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-TinyTransformer_1.0-stdout
2023-10-04T14:45:14,586 [INFO ] W-9000-TinyTransformer_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-TinyTransformer_1.0-stderr
2023-10-04T14:50:19,784 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2023-10-04T14:50:19,784 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2023-10-04T14:50:19,812 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2023-10-04T14:50:19,812 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2023-10-04T14:50:19,812 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2023-10-04T14:50:19,812 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2023-10-04T14:50:19,839 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml
2023-10-04T14:50:19,839 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml
2023-10-04T14:50:19,943 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.8.2
TS Home: /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages
Current directory: /home/mathadoor/Documents/transformer-exposition-backend/app
Temp directory: /tmp
Metrics config path: /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 32
Max heap size: 16000 M
Python executable: /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/bin/python3.9
Config file: logs/config/20231004144515906-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/mathadoor/Documents/transformer-exposition-backend/app
Initial Models: TinyTransformer.mar
Log dir: /home/mathadoor/Documents/transformer-exposition-backend/app/logs
Metrics dir: /home/mathadoor/Documents/transformer-exposition-backend/app/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: log
Disable system metrics: false
Workflow Store: /home/mathadoor/Documents/transformer-exposition-backend/app
Model config: N/A
2023-10-04T14:50:19,943 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.8.2
TS Home: /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages
Current directory: /home/mathadoor/Documents/transformer-exposition-backend/app
Temp directory: /tmp
Metrics config path: /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 32
Max heap size: 16000 M
Python executable: /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/bin/python3.9
Config file: logs/config/20231004144515906-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/mathadoor/Documents/transformer-exposition-backend/app
Initial Models: TinyTransformer.mar
Log dir: /home/mathadoor/Documents/transformer-exposition-backend/app/logs
Metrics dir: /home/mathadoor/Documents/transformer-exposition-backend/app/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: log
Disable system metrics: false
Workflow Store: /home/mathadoor/Documents/transformer-exposition-backend/app
Model config: N/A
2023-10-04T14:50:19,952 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20231004144515906-shutdown.cfg",
  "modelCount": 1,
  "created": 1696445115906,
  "models": {
    "TinyTransformer": {
      "1.0": {
        "defaultVersion": true,
        "marName": "TinyTransformer.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2023-10-04T14:50:19,952 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20231004144515906-shutdown.cfg",
  "modelCount": 1,
  "created": 1696445115906,
  "models": {
    "TinyTransformer": {
      "1.0": {
        "defaultVersion": true,
        "marName": "TinyTransformer.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2023-10-04T14:50:19,959 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20231004144515906-shutdown.cfg
2023-10-04T14:50:19,959 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20231004144515906-shutdown.cfg
2023-10-04T14:50:19,959 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20231004144515906-shutdown.cfg validated successfully
2023-10-04T14:50:19,959 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20231004144515906-shutdown.cfg validated successfully
2023-10-04T14:50:20,114 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model TinyTransformer
2023-10-04T14:50:20,114 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model TinyTransformer
2023-10-04T14:50:20,114 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model TinyTransformer
2023-10-04T14:50:20,114 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model TinyTransformer
2023-10-04T14:50:20,114 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model TinyTransformer
2023-10-04T14:50:20,114 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model TinyTransformer
2023-10-04T14:50:20,114 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model TinyTransformer loaded.
2023-10-04T14:50:20,114 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model TinyTransformer loaded.
2023-10-04T14:50:20,114 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: TinyTransformer, count: 1
2023-10-04T14:50:20,114 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: TinyTransformer, count: 1
2023-10-04T14:50:20,119 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2023-10-04T14:50:20,118 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/bin/python3.9, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml]
2023-10-04T14:50:20,119 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2023-10-04T14:50:20,118 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/bin/python3.9, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml]
2023-10-04T14:50:20,151 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2023-10-04T14:50:20,151 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2023-10-04T14:50:20,151 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2023-10-04T14:50:20,151 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2023-10-04T14:50:20,151 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2023-10-04T14:50:20,151 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2023-10-04T14:50:20,151 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2023-10-04T14:50:20,151 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2023-10-04T14:50:20,152 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2023-10-04T14:50:20,152 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2023-10-04T14:50:20,267 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2023-10-04T14:50:20,267 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2023-10-04T14:50:20,301 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2023-10-04T14:50:20,301 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2023-10-04T14:50:20,832 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=21688
2023-10-04T14:50:20,832 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-10-04T14:50:20,858 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Successfully loaded /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml.
2023-10-04T14:50:20,858 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - [PID]21688
2023-10-04T14:50:20,858 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Torch worker started.
2023-10-04T14:50:20,858 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Python runtime: 3.9.18
2023-10-04T14:50:20,859 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change null -> WORKER_STARTED
2023-10-04T14:50:20,859 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change null -> WORKER_STARTED
2023-10-04T14:50:20,862 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-10-04T14:50:20,862 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-10-04T14:50:20,868 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2023-10-04T14:50:20,870 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD to backend at: 1696445420870
2023-10-04T14:50:20,870 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD to backend at: 1696445420870
2023-10-04T14:50:20,891 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - model_name: TinyTransformer, batchSize: 1
2023-10-04T14:50:20,917 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - proceeding without onnxruntime
2023-10-04T14:50:20,918 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2023-10-04T14:50:20,928 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Failed to load model TinyTransformer, exception PytorchStreamReader failed locating file constants.pkl: file not found
2023-10-04T14:50:20,928 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2023-10-04T14:50:20,928 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG -   File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/model_service_worker.py", line 131, in load_model
2023-10-04T14:50:20,928 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG -     service = model_loader.load(
2023-10-04T14:50:20,928 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG -   File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/model_loader.py", line 135, in load
2023-10-04T14:50:20,928 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2023-10-04T14:50:20,928 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG -   File "/tmp/models/e645884693ed44e994ed71697704996b/TranslationHandler.py", line 60, in initialize
2023-10-04T14:50:20,929 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG -     self.model = torch.jit.load(model_pt_path)
2023-10-04T14:50:20,929 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG -   File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/torch/jit/_serialization.py", line 162, in load
2023-10-04T14:50:20,929 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG -     cpp_module = torch._C.import_ir_module(cu, str(f), map_location, _extra_files, _restore_shapes)  # type: ignore[call-arg]
2023-10-04T14:50:20,929 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - RuntimeError: PytorchStreamReader failed locating file constants.pkl: file not found
2023-10-04T14:50:20,929 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Backend worker process died.
2023-10-04T14:50:20,929 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2023-10-04T14:50:20,929 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG -   File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/model_service_worker.py", line 253, in <module>
2023-10-04T14:50:20,929 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG -     worker.run_server()
2023-10-04T14:50:20,929 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG -   File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/model_service_worker.py", line 221, in run_server
2023-10-04T14:50:20,929 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2023-10-04T14:50:20,929 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG -   File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/model_service_worker.py", line 189, in handle_connection
2023-10-04T14:50:20,929 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG -     raise RuntimeError("{} - {}".format(code, result))
2023-10-04T14:50:20,930 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - RuntimeError: 500 - Unknown exception
2023-10-04T14:50:20,932 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - sent a reply, jobdone: true
2023-10-04T14:50:20,932 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - sent a reply, jobdone: true
2023-10-04T14:50:20,932 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 41
2023-10-04T14:50:20,932 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 41
2023-10-04T14:50:20,933 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change WORKER_STARTED -> WORKER_ERROR
2023-10-04T14:50:20,933 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-10-04T14:50:20,933 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change WORKER_STARTED -> WORKER_ERROR
2023-10-04T14:50:20,933 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-10-04T14:50:20,934 [INFO ] W-9000-TinyTransformer_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:22.0|#Level:Host|#hostname:berserker,timestamp:1696445420
2023-10-04T14:50:20,935 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_ERROR
2023-10-04T14:50:20,935 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_ERROR
2023-10-04T14:50:20,935 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.ReentrantLock$Sync.lockInterruptibly(ReentrantLock.java:159) ~[?:?]
	at java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:372) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:272) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:34) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:186) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2023-10-04T14:50:20,935 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.ReentrantLock$Sync.lockInterruptibly(ReentrantLock.java:159) ~[?:?]
	at java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:372) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:272) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:34) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:186) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2023-10-04T14:50:20,940 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change WORKER_ERROR -> WORKER_STOPPED
2023-10-04T14:50:20,940 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change WORKER_ERROR -> WORKER_STOPPED
2023-10-04T14:50:20,940 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1696445420940
2023-10-04T14:50:20,940 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1696445420940
2023-10-04T14:50:20,940 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-TinyTransformer_1.0-stderr
2023-10-04T14:50:20,940 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-TinyTransformer_1.0-stderr
2023-10-04T14:50:20,940 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-TinyTransformer_1.0-stdout
2023-10-04T14:50:20,940 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-TinyTransformer_1.0-stdout
2023-10-04T14:50:20,940 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2023-10-04T14:50:20,940 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2023-10-04T14:50:20,946 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-TinyTransformer_1.0-stdout
2023-10-04T14:50:20,946 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-TinyTransformer_1.0-stdout
2023-10-04T14:50:20,946 [INFO ] W-9000-TinyTransformer_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-TinyTransformer_1.0-stderr
2023-10-04T14:50:20,946 [INFO ] W-9000-TinyTransformer_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-TinyTransformer_1.0-stderr
2023-10-04T14:50:21,941 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/bin/python3.9, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml]
2023-10-04T14:50:21,941 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/bin/python3.9, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml]
2023-10-04T14:50:22,669 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=21759
2023-10-04T14:50:22,669 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-10-04T14:50:22,696 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Successfully loaded /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml.
2023-10-04T14:50:22,696 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - [PID]21759
2023-10-04T14:50:22,696 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Torch worker started.
2023-10-04T14:50:22,696 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Python runtime: 3.9.18
2023-10-04T14:50:22,696 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-10-04T14:50:22,696 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-10-04T14:50:22,696 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-10-04T14:50:22,696 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-10-04T14:50:22,697 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-10-04T14:50:22,697 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-10-04T14:50:22,697 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2023-10-04T14:50:22,697 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-10-04T14:50:22,697 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-10-04T14:50:22,698 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.pollFirst(LinkedBlockingDeque.java:515) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.poll(LinkedBlockingDeque.java:677) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:276) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:34) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:186) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2023-10-04T14:50:22,698 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.pollFirst(LinkedBlockingDeque.java:515) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.poll(LinkedBlockingDeque.java:677) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:276) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:34) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:186) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2023-10-04T14:50:22,698 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-10-04T14:50:22,698 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-10-04T14:50:22,698 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-10-04T14:50:22,698 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-10-04T14:50:22,698 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-TinyTransformer_1.0-stderr
2023-10-04T14:50:22,698 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-TinyTransformer_1.0-stderr
2023-10-04T14:50:22,698 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-TinyTransformer_1.0-stdout
2023-10-04T14:50:22,698 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-TinyTransformer_1.0-stdout
2023-10-04T14:50:22,698 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2023-10-04T14:50:22,698 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2023-10-04T14:50:22,705 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-TinyTransformer_1.0-stdout
2023-10-04T14:50:22,705 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-TinyTransformer_1.0-stdout
2023-10-04T14:50:22,705 [INFO ] W-9000-TinyTransformer_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-TinyTransformer_1.0-stderr
2023-10-04T14:50:22,705 [INFO ] W-9000-TinyTransformer_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-TinyTransformer_1.0-stderr
2023-10-04T14:50:23,699 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/bin/python3.9, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml]
2023-10-04T14:50:23,699 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/bin/python3.9, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml]
2023-10-04T14:50:24,418 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=21799
2023-10-04T14:50:24,418 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-10-04T14:50:24,446 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Successfully loaded /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml.
2023-10-04T14:50:24,447 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - [PID]21799
2023-10-04T14:50:24,447 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Torch worker started.
2023-10-04T14:50:24,447 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Python runtime: 3.9.18
2023-10-04T14:50:24,447 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-10-04T14:50:24,447 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-10-04T14:50:24,447 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-10-04T14:50:24,447 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-10-04T14:50:24,448 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-10-04T14:50:24,448 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-10-04T14:50:24,448 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2023-10-04T14:50:24,449 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-10-04T14:50:24,449 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-10-04T14:50:24,449 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.ReentrantLock$Sync.lockInterruptibly(ReentrantLock.java:159) ~[?:?]
	at java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:372) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:272) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:34) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:186) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2023-10-04T14:50:24,449 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.ReentrantLock$Sync.lockInterruptibly(ReentrantLock.java:159) ~[?:?]
	at java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:372) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:272) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:34) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:186) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2023-10-04T14:50:24,449 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-10-04T14:50:24,449 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-10-04T14:50:24,449 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-10-04T14:50:24,449 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-10-04T14:50:24,449 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-TinyTransformer_1.0-stderr
2023-10-04T14:50:24,449 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-TinyTransformer_1.0-stderr
2023-10-04T14:50:24,449 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-TinyTransformer_1.0-stdout
2023-10-04T14:50:24,449 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-TinyTransformer_1.0-stdout
2023-10-04T14:50:24,449 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2023-10-04T14:50:24,449 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.
2023-10-04T14:50:24,456 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-TinyTransformer_1.0-stdout
2023-10-04T14:50:24,456 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-TinyTransformer_1.0-stdout
2023-10-04T14:50:24,456 [INFO ] W-9000-TinyTransformer_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-TinyTransformer_1.0-stderr
2023-10-04T14:50:24,456 [INFO ] W-9000-TinyTransformer_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-TinyTransformer_1.0-stderr
2023-10-04T14:50:48,566 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2023-10-04T14:50:48,566 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2023-10-04T14:50:48,594 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2023-10-04T14:50:48,594 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2023-10-04T14:50:48,595 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2023-10-04T14:50:48,595 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2023-10-04T14:50:48,621 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml
2023-10-04T14:50:48,621 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml
2023-10-04T14:50:48,752 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.8.2
TS Home: /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages
Current directory: /home/mathadoor/Documents/transformer-exposition-backend/app
Temp directory: /tmp
Metrics config path: /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 32
Max heap size: 16000 M
Python executable: /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/bin/python3.9
Config file: logs/config/20231004145024675-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/mathadoor/Documents/transformer-exposition-backend/app
Initial Models: TinyTransformer.mar
Log dir: /home/mathadoor/Documents/transformer-exposition-backend/app/logs
Metrics dir: /home/mathadoor/Documents/transformer-exposition-backend/app/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: log
Disable system metrics: false
Workflow Store: /home/mathadoor/Documents/transformer-exposition-backend/app
Model config: N/A
2023-10-04T14:50:48,752 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.8.2
TS Home: /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages
Current directory: /home/mathadoor/Documents/transformer-exposition-backend/app
Temp directory: /tmp
Metrics config path: /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml
Number of GPUs: 1
Number of CPUs: 32
Max heap size: 16000 M
Python executable: /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/bin/python3.9
Config file: logs/config/20231004145024675-shutdown.cfg
Inference address: http://127.0.0.1:8080
Management address: http://127.0.0.1:8081
Metrics address: http://127.0.0.1:8082
Model Store: /home/mathadoor/Documents/transformer-exposition-backend/app
Initial Models: TinyTransformer.mar
Log dir: /home/mathadoor/Documents/transformer-exposition-backend/app/logs
Metrics dir: /home/mathadoor/Documents/transformer-exposition-backend/app/logs
Netty threads: 0
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: false
Enable metrics API: true
Metrics mode: log
Disable system metrics: false
Workflow Store: /home/mathadoor/Documents/transformer-exposition-backend/app
Model config: N/A
2023-10-04T14:50:48,760 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20231004145024675-shutdown.cfg",
  "modelCount": 1,
  "created": 1696445424675,
  "models": {
    "TinyTransformer": {
      "1.0": {
        "defaultVersion": true,
        "marName": "TinyTransformer.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2023-10-04T14:50:48,760 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {
  "name": "20231004145024675-shutdown.cfg",
  "modelCount": 1,
  "created": 1696445424675,
  "models": {
    "TinyTransformer": {
      "1.0": {
        "defaultVersion": true,
        "marName": "TinyTransformer.mar",
        "minWorkers": 1,
        "maxWorkers": 1,
        "batchSize": 1,
        "maxBatchDelay": 100,
        "responseTimeout": 120
      }
    }
  }
}
2023-10-04T14:50:48,766 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20231004145024675-shutdown.cfg
2023-10-04T14:50:48,766 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20231004145024675-shutdown.cfg
2023-10-04T14:50:48,767 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20231004145024675-shutdown.cfg validated successfully
2023-10-04T14:50:48,767 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20231004145024675-shutdown.cfg validated successfully
2023-10-04T14:50:48,923 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model TinyTransformer
2023-10-04T14:50:48,923 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model TinyTransformer
2023-10-04T14:50:48,923 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model TinyTransformer
2023-10-04T14:50:48,923 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model TinyTransformer
2023-10-04T14:50:48,923 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model TinyTransformer
2023-10-04T14:50:48,923 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model TinyTransformer
2023-10-04T14:50:48,923 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model TinyTransformer loaded.
2023-10-04T14:50:48,923 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model TinyTransformer loaded.
2023-10-04T14:50:48,923 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: TinyTransformer, count: 1
2023-10-04T14:50:48,923 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: TinyTransformer, count: 1
2023-10-04T14:50:48,928 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2023-10-04T14:50:48,928 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2023-10-04T14:50:48,928 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/bin/python3.9, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml]
2023-10-04T14:50:48,928 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/bin/python3.9, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml]
2023-10-04T14:50:48,960 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2023-10-04T14:50:48,960 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8080
2023-10-04T14:50:48,960 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2023-10-04T14:50:48,960 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2023-10-04T14:50:48,961 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2023-10-04T14:50:48,961 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8081
2023-10-04T14:50:48,961 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2023-10-04T14:50:48,961 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2023-10-04T14:50:48,961 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2023-10-04T14:50:48,961 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082
2023-10-04T14:50:49,087 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2023-10-04T14:50:49,087 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
2023-10-04T14:50:49,121 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2023-10-04T14:50:49,121 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):
  File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/metrics/metric_collector.py", line 27, in <module>
    system_metrics.collect_all(sys.modules['ts.metrics.system_metrics'], arguments.gpu)
  File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/metrics/system_metrics.py", line 119, in collect_all
    value(num_of_gpu)
  File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/metrics/system_metrics.py", line 64, in gpu_utilization
    import nvgpu
ModuleNotFoundError: No module named 'nvgpu'

2023-10-04T14:50:49,644 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=22034
2023-10-04T14:50:49,645 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-10-04T14:50:49,671 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Successfully loaded /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml.
2023-10-04T14:50:49,671 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - [PID]22034
2023-10-04T14:50:49,671 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Torch worker started.
2023-10-04T14:50:49,671 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Python runtime: 3.9.18
2023-10-04T14:50:49,672 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change null -> WORKER_STARTED
2023-10-04T14:50:49,672 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change null -> WORKER_STARTED
2023-10-04T14:50:49,674 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-10-04T14:50:49,674 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-10-04T14:50:49,681 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2023-10-04T14:50:49,683 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD to backend at: 1696445449683
2023-10-04T14:50:49,683 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD to backend at: 1696445449683
2023-10-04T14:50:49,699 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - model_name: TinyTransformer, batchSize: 1
2023-10-04T14:50:49,727 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - proceeding without onnxruntime
2023-10-04T14:50:49,727 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Torch TensorRT not enabled
2023-10-04T14:50:49,728 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Backend worker process died.
2023-10-04T14:50:49,728 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Traceback (most recent call last):
2023-10-04T14:50:49,728 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG -   File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/model_service_worker.py", line 253, in <module>
2023-10-04T14:50:49,728 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG -     worker.run_server()
2023-10-04T14:50:49,728 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-10-04T14:50:49,728 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-10-04T14:50:49,729 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG -   File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/model_service_worker.py", line 221, in run_server
2023-10-04T14:50:49,729 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)
2023-10-04T14:50:49,729 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-10-04T14:50:49,729 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG -   File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/model_service_worker.py", line 184, in handle_connection
2023-10-04T14:50:49,729 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-10-04T14:50:49,729 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)
2023-10-04T14:50:49,729 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG -   File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/model_service_worker.py", line 131, in load_model
2023-10-04T14:50:49,729 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG -     service = model_loader.load(
2023-10-04T14:50:49,730 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG -   File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/model_loader.py", line 135, in load
2023-10-04T14:50:49,730 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG -     initialize_fn(service.context)
2023-10-04T14:50:49,730 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG -   File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/torch_handler/base_handler.py", line 166, in initialize
2023-10-04T14:50:49,730 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG -     self.model = self._load_pickled_model(
2023-10-04T14:50:49,730 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG -   File "/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/torch_handler/base_handler.py", line 262, in _load_pickled_model
2023-10-04T14:50:49,730 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG -     model = model_class()
2023-10-04T14:50:49,730 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - TypeError: __init__() missing 11 required positional arguments: 'embedding_size', 'src_vocab_size', 'trg_vocab_size', 'src_pad_idx', 'num_heads', 'num_encoder_layers', 'num_decoder_layers', 'feedforward_dim', 'dropout', 'max_len', and 'device'
2023-10-04T14:50:49,729 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:213) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2023-10-04T14:50:49,729 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:435) ~[?:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:213) [model-server.jar:?]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) [?:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2023-10-04T14:50:49,736 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: TinyTransformer, error: Worker died.
2023-10-04T14:50:49,736 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: TinyTransformer, error: Worker died.
2023-10-04T14:50:49,736 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-10-04T14:50:49,736 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-10-04T14:50:49,737 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1696445449736
2023-10-04T14:50:49,737 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1696445449736
2023-10-04T14:50:49,737 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-TinyTransformer_1.0-stderr
2023-10-04T14:50:49,737 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-TinyTransformer_1.0-stderr
2023-10-04T14:50:49,737 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-TinyTransformer_1.0-stdout
2023-10-04T14:50:49,737 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-TinyTransformer_1.0-stdout
2023-10-04T14:50:49,737 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2023-10-04T14:50:49,737 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2023-10-04T14:50:49,743 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-TinyTransformer_1.0-stdout
2023-10-04T14:50:49,743 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-TinyTransformer_1.0-stdout
2023-10-04T14:50:49,743 [INFO ] W-9000-TinyTransformer_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-TinyTransformer_1.0-stderr
2023-10-04T14:50:49,743 [INFO ] W-9000-TinyTransformer_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-TinyTransformer_1.0-stderr
2023-10-04T14:50:50,738 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/bin/python3.9, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml]
2023-10-04T14:50:50,738 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/bin/python3.9, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml]
2023-10-04T14:50:51,445 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=22104
2023-10-04T14:50:51,446 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000
2023-10-04T14:50:51,471 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Successfully loaded /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml.
2023-10-04T14:50:51,471 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - [PID]22104
2023-10-04T14:50:51,471 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Torch worker started.
2023-10-04T14:50:51,471 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-10-04T14:50:51,471 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Python runtime: 3.9.18
2023-10-04T14:50:51,471 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change WORKER_STOPPED -> WORKER_STARTED
2023-10-04T14:50:51,471 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-10-04T14:50:51,471 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000
2023-10-04T14:50:51,472 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-10-04T14:50:51,472 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED
2023-10-04T14:50:51,472 [INFO ] W-9000-TinyTransformer_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.
2023-10-04T14:50:51,472 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-10-04T14:50:51,472 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED
2023-10-04T14:50:51,472 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.pollFirst(LinkedBlockingDeque.java:515) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.poll(LinkedBlockingDeque.java:677) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:276) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:34) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:186) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2023-10-04T14:50:51,472 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died.
java.lang.InterruptedException: null
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1679) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.pollFirst(LinkedBlockingDeque.java:515) ~[?:?]
	at java.util.concurrent.LinkedBlockingDeque.poll(LinkedBlockingDeque.java:677) ~[?:?]
	at org.pytorch.serve.wlm.Model.pollBatch(Model.java:276) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.BatchAggregator.getRequest(BatchAggregator.java:34) ~[model-server.jar:?]
	at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:186) [model-server.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]
	at java.lang.Thread.run(Thread.java:833) [?:?]
2023-10-04T14:50:51,473 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-10-04T14:50:51,473 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-TinyTransformer_1.0 State change WORKER_STARTED -> WORKER_STOPPED
2023-10-04T14:50:51,473 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-10-04T14:50:51,473 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again
2023-10-04T14:50:51,473 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-TinyTransformer_1.0-stderr
2023-10-04T14:50:51,473 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-TinyTransformer_1.0-stderr
2023-10-04T14:50:51,473 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-TinyTransformer_1.0-stdout
2023-10-04T14:50:51,473 [WARN ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-TinyTransformer_1.0-stdout
2023-10-04T14:50:51,473 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2023-10-04T14:50:51,473 [INFO ] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.
2023-10-04T14:50:51,479 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-TinyTransformer_1.0-stdout
2023-10-04T14:50:51,479 [INFO ] W-9000-TinyTransformer_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-TinyTransformer_1.0-stdout
2023-10-04T14:50:51,479 [INFO ] W-9000-TinyTransformer_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-TinyTransformer_1.0-stderr
2023-10-04T14:50:51,479 [INFO ] W-9000-TinyTransformer_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-TinyTransformer_1.0-stderr
2023-10-04T14:50:52,474 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/bin/python3.9, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml]
2023-10-04T14:50:52,474 [DEBUG] W-9000-TinyTransformer_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/bin/python3.9, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /home/mathadoor/miniconda3/envs/bcos/envs/transformerExposition/lib/python3.9/site-packages/ts/configs/metrics.yaml]
